{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from random import randrange\n",
    "from pandasql import sqldf\n",
    "import awswrangler as wr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_new_file = pd.read_csv('s3://stock-market-raw-data-us-east-1/stg_price_by_date/test_data.csv')\n",
    "df_new_file = pd.read_csv('s3://stock-market-raw-data-us-east-1/stg_price_by_date/20250131175440.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_athena = wr.athena.read_sql_query('SELECT * FROM price_by_date', database = 'stock_market')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_records = sqldf(\n",
    "         '''select a.company company, a.date as close_date, a.close_price   \n",
    "            from df_new_file a \n",
    "            left join df_athena b on a.company = b.company and \n",
    "                a.date = b.close_date\n",
    "            where b.company is null\n",
    "         '''\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert string to date python date object\n",
    "\n",
    "df_new_records['close_date'] = df_new_records['close_date'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d').date()) \n",
    "\n",
    "#OR\n",
    "\n",
    "#df_new_records['close_date'] = pd.to_datetime(df_new_records['close_date']).dt.date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the partition column\n",
    "df_new_records['p_year'] = pd.to_datetime(df_new_records['close_date']).dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the DataFrame to a PyArrow Table\n",
    "table = pa.Table.from_pandas(df_new_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to Parquet with partitioning by year\n",
    "s3_path = 's3://stock-market-raw-data-us-east-1/price_by_date/'\n",
    "pq.write_to_dataset(table, root_path = s3_path, partition_cols=['p_year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch_files(path):\n",
    "    \n",
    "    import pandas as pd\n",
    "    from datetime import datetime, timedelta\n",
    "    from random import randrange\n",
    "    from pyathena import connect\n",
    "    from pandasql import sqldf\n",
    "    import pyarrow as pa\n",
    "    import pyarrow.parquet as pq\n",
    "    \n",
    "    df_new_file = pd.read_csv(path)\n",
    "    \n",
    "    conn = connect(s3_staging_dir='s3://de-youtube-project-useast-dev/',region_name='us-east-1')\n",
    "    df_athena = pd.read_sql('select * from AwsDataCatalog.stock_market.price_by_date', conn)\n",
    "    \n",
    "    df_new_records = sqldf(\n",
    "         '''select a.company company, a.date as close_date, a.close_price   \n",
    "            from df_new_file a \n",
    "            left join df_athena b on a.company = b.company and \n",
    "                a.date = b.close_date\n",
    "            where b.company is null\n",
    "         '''\n",
    "     )\n",
    "    \n",
    "    df_new_records['close_date'] = df_new_records['close_date'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d').date()) \n",
    "    df_new_records['p_year'] = pd.to_datetime(df_new_records['close_date']).dt.year\n",
    "    \n",
    "    table = pa.Table.from_pandas(df_new_records)\n",
    "    s3_path = 's3://stock-market-raw-data-us-east-1/price_by_date/'\n",
    "    pq.write_to_dataset(table, root_path = s3_path, partition_cols=['p_year'])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-542-14245a7c672c>:14: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_athena = pd.read_sql('select * from AwsDataCatalog.stock_market.price_by_date', conn)\n"
     ]
    }
   ],
   "source": [
    "path = 's3://stock-market-raw-data-us-east-1/stg_price_by_date/20250131163557.csv'\n",
    "process_batch_files(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3://stock-market-raw-data-us-east-1/stg_price_by_date/20250131172624.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch_files_lambda(event, context):\n",
    "    \n",
    "    import pandas as pd\n",
    "    from datetime import datetime, timedelta\n",
    "    from random import randrange\n",
    "    from pandasql import sqldf\n",
    "    import awswrangler as wr\n",
    "\n",
    "    \n",
    "    source_bucket = event['Records'][0]['s3']['bucket']['name']\n",
    "    key = event['Records'][0]['s3']['object']['key']\n",
    "    path = source_bucket + key\n",
    "    \n",
    "    df_new_file = pd.read_csv(path)\n",
    "    \n",
    "    df_athena = wr.athena.read_sql_query('SELECT * FROM price_by_date', database = 'stock_market')\n",
    "    \n",
    "    df_new_records = sqldf(\n",
    "         '''select a.company company, a.date as close_date, a.close_price   \n",
    "            from df_new_file a \n",
    "            left join df_athena b on a.company = b.company and \n",
    "                a.date = b.close_date\n",
    "            where b.company is null\n",
    "         '''\n",
    "     )\n",
    "    \n",
    "    df_new_records['close_date'] = df_new_records['close_date'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d').date()) \n",
    "    df_new_records['p_year'] = pd.to_datetime(df_new_records['close_date']).dt.year\n",
    "    \n",
    "    df_new_records.to_parquet(path = 's3://stock-market-raw-data-us-east-1/price_by_date/', index = False, \\\n",
    "                                partition_cols=['p_year'])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "event = \\\n",
    "{  \n",
    "   \"Records\":[  \n",
    "      {  \n",
    "         \"eventVersion\":\"2.2\",\n",
    "         \"eventSource\":\"aws:s3\",\n",
    "         \"awsRegion\":\"us-west-2\",\n",
    "         \"eventTime\":\"The time, in ISO-8601 format, for example, 1970-01-01T00:00:00.000Z, when Amazon S3 finished processing the request\",\n",
    "         \"eventName\":\"event-type\",\n",
    "         \"userIdentity\":{  \n",
    "            \"principalId\":\"Amazon-customer-ID-of-the-user-who-caused-the-event\"\n",
    "         },\n",
    "         \"requestParameters\":{  \n",
    "            \"sourceIPAddress\":\"ip-address-where-request-came-from\"\n",
    "         },\n",
    "         \"responseElements\":{  \n",
    "            \"x-amz-request-id\":\"Amazon S3 generated request ID\",\n",
    "            \"x-amz-id-2\":\"Amazon S3 host that processed the request\"\n",
    "         },\n",
    "         \"s3\":{  \n",
    "            \"s3SchemaVersion\":\"1.0\",\n",
    "            \"configurationId\":\"ID found in the bucket notification configuration\",\n",
    "            \"bucket\":{  \n",
    "               \"name\":\"s3://stock-market-raw-data-us-east-1/\",\n",
    "               \"ownerIdentity\":{  \n",
    "                  \"principalId\":\"Amazon-customer-ID-of-the-bucket-owner\"\n",
    "               },\n",
    "               \"arn\":\"arn:aws:s3:::stock-market-raw-data-us-east-1\"\n",
    "            },\n",
    "            \"object\":{  \n",
    "               \"key\":\"stg_price_by_date/20250131192045.csv\",\n",
    "               \"size\":\"object-size in bytes\",\n",
    "               \"eTag\":\"object eTag\",\n",
    "               \"versionId\":\"object version if bucket is versioning-enabled, otherwise null\",\n",
    "               \"sequencer\": \"a string representation of a hexadecimal value used to determine event sequence, only used with PUTs and DELETEs\"\n",
    "            }\n",
    "         },\n",
    "         \"glacierEventData\": {\n",
    "            \"restoreEventData\": {\n",
    "               \"lifecycleRestorationExpiryTime\": \"The time, in ISO-8601 format, for example, 1970-01-01T00:00:00.000Z, of Restore Expiry\",\n",
    "               \"lifecycleRestoreStorageClass\": \"Source storage class for restore\"\n",
    "            }\n",
    "         }\n",
    "      }\n",
    "   ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = ''\n",
    "process_batch_files_lambda(event, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
