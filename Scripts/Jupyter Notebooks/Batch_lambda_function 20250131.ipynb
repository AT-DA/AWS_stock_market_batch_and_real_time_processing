{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from random import randrange\n",
    "import s3fs\n",
    "from pyathena import connect\n",
    "from pandasql import sqldf\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_new_file = pd.read_csv('s3://stock-market-raw-data-us-east-1/stg_price_by_date/test_data.csv')\n",
    "df_new_file = pd.read_csv('s3://stock-market-raw-data-us-east-1/stg_price_by_date/20250130000719.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-532-8357164e96e7>:2: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_athena = pd.read_sql('select * from AwsDataCatalog.stock_market.price_by_date', conn)\n"
     ]
    }
   ],
   "source": [
    "conn = connect(s3_staging_dir='s3://de-youtube-project-useast-dev/',region_name='us-east-1')\n",
    "df_athena = pd.read_sql('select * from AwsDataCatalog.stock_market.price_by_date', conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_records = sqldf(\n",
    "         '''select a.company company, a.date as close_date, a.close_price   \n",
    "            from df_new_file a \n",
    "            left join df_athena b on a.company = b.company and \n",
    "                a.date = b.close_date\n",
    "            where b.company is null\n",
    "         '''\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert string to date python date object\n",
    "\n",
    "df_new_records['close_date'] = df_new_records['close_date'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d').date()) \n",
    "\n",
    "#OR\n",
    "\n",
    "#df_new_records['close_date'] = pd.to_datetime(df_new_records['close_date']).dt.date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the partition column\n",
    "df_new_records['p_year'] = pd.to_datetime(df_new_records['close_date']).dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the DataFrame to a PyArrow Table\n",
    "table = pa.Table.from_pandas(df_new_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to Parquet with partitioning by year\n",
    "s3_path = 's3://stock-market-raw-data-us-east-1/price_by_date/'\n",
    "pq.write_to_dataset(table, root_path = s3_path, partition_cols=['p_year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch_files(path):\n",
    "    \n",
    "    import pandas as pd\n",
    "    from datetime import datetime, timedelta\n",
    "    from random import randrange\n",
    "    import s3fs\n",
    "    from pyathena import connect\n",
    "    from pandasql import sqldf\n",
    "    import pyarrow as pa\n",
    "    import pyarrow.parquet as pq\n",
    "    \n",
    "    df_new_file = pd.read_csv(path)\n",
    "    \n",
    "    conn = connect(s3_staging_dir='s3://de-youtube-project-useast-dev/',region_name='us-east-1')\n",
    "    df_athena = pd.read_sql('select * from AwsDataCatalog.stock_market.price_by_date', conn)\n",
    "    \n",
    "    df_new_records = sqldf(\n",
    "         '''select a.company company, a.date as close_date, a.close_price   \n",
    "            from df_new_file a \n",
    "            left join df_athena b on a.company = b.company and \n",
    "                a.date = b.close_date\n",
    "            where b.company is null\n",
    "         '''\n",
    "     )\n",
    "    \n",
    "    df_new_records['close_date'] = df_new_records['close_date'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d').date()) \n",
    "    df_new_records['p_year'] = pd.to_datetime(df_new_records['close_date']).dt.year\n",
    "    \n",
    "    table = pa.Table.from_pandas(df_new_records)\n",
    "    s3_path = 's3://stock-market-raw-data-us-east-1/price_by_date/'\n",
    "    pq.write_to_dataset(table, root_path = s3_path, partition_cols=['p_year'])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-539-04455442d491>:15: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_athena = pd.read_sql('select * from AwsDataCatalog.stock_market.price_by_date', conn)\n"
     ]
    }
   ],
   "source": [
    "path = 's3://stock-market-raw-data-us-east-1/stg_price_by_date/20250131163557.csv'\n",
    "process_batch_files(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
